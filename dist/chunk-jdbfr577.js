import{e as n,h as a}from"/dist/chunk-n2t7x54v.js";function l(){return a("div",{class:"markdown-body",children:a(n,{children:[a("h1",{children:"llama.cpp 驱动 baichuan2-7b-chat"},void 0,!1,void 0,this),`
`,a("p",{children:"需要 python3.10、llama.cpp 源码包，llama.cpp 二进制版本。"},void 0,!1,void 0,this),`
`,a("p",{children:["下面使用的是 ",a("a",{href:"https://github.com/ggerganov/llama.cpp/releases/tag/b1414",children:"llama.cpp b1414"},void 0,!1,void 0,this)]},void 0,!0,void 0,this),`
`,a("p",{children:["工作目录在 ",a("code",{children:"D:\\llm_store"},void 0,!1,void 0,this),"。"]},void 0,!0,void 0,this),`
`,a("h2",{children:"下载模型"},void 0,!1,void 0,this),`
`,a("p",{children:["Baichuan2-7B-Chat 可以在 ",a("a",{href:"https://huggingface.co/baichuan-inc/Baichuan2-7B-Chat",children:"HF"},void 0,!1,void 0,this)," 下载。如果有网络故障，可以上网找找镜像之类的备用下载方式。"]},void 0,!0,void 0,this),`
`,a("h2",{children:"转换到 baichuan1 模型"},void 0,!1,void 0,this),`
`,a("p",{children:"llama.cpp 官方已经支持 baichuan 模型。但当前版本 b1414 只支持 baichuan1 模型。baichuan2 模型需要转换为 baichuan1 的模型格式。"},void 0,!1,void 0,this),`
`,a("p",{children:"用 baichuan 提供的脚本进行处理："},void 0,!1,void 0,this),`
`,a("pre",{className:"shiki github-dark",style:{backgroundColor:"#24292e",color:"#e1e4e8"},tabIndex:"0",children:a("code",{className:"language-sh",children:[a("span",{className:"line",children:[a("span",{style:{color:"#B392F0"},children:"import"},void 0,!1,void 0,this),a("span",{style:{color:"#9ECBFF"},children:" torch"},void 0,!1,void 0,this)]},void 0,!0,void 0,this),`
`,a("span",{className:"line",children:[a("span",{style:{color:"#B392F0"},children:"import"},void 0,!1,void 0,this),a("span",{style:{color:"#9ECBFF"},children:" os"},void 0,!1,void 0,this)]},void 0,!0,void 0,this),`
`,a("span",{className:"line",children:[a("span",{style:{color:"#B392F0"},children:"ori_model_dir"},void 0,!1,void 0,this),a("span",{style:{color:"#9ECBFF"},children:" ="},void 0,!1,void 0,this),a("span",{style:{color:"#9ECBFF"},children:" 'D:\\\\llm_store\\\\baichuan2\\\\Baichuan2-7B-Chat'"},void 0,!1,void 0,this)]},void 0,!0,void 0,this),`
`,a("span",{className:"line",children:a("span",{style:{color:"#6A737D"},children:"# To avoid overwriting the original model, it's best to save the converted model to another directory before replacing it"},void 0,!1,void 0,this)},void 0,!1,void 0,this),`
`,a("span",{className:"line",children:[a("span",{style:{color:"#B392F0"},children:"new_model_dir"},void 0,!1,void 0,this),a("span",{style:{color:"#9ECBFF"},children:" ="},void 0,!1,void 0,this),a("span",{style:{color:"#9ECBFF"},children:" 'D:\\\\llm_store\\\\baichuan2\\\\Baichuan2-7B-Chat-bc1'"},void 0,!1,void 0,this)]},void 0,!0,void 0,this),`
`,a("span",{className:"line",children:[a("span",{style:{color:"#B392F0"},children:"model"},void 0,!1,void 0,this),a("span",{style:{color:"#9ECBFF"},children:" ="},void 0,!1,void 0,this),a("span",{style:{color:"#9ECBFF"},children:" torch.load"},void 0,!1,void 0,this),a("span",{style:{color:"#E1E4E8"},children:"("},void 0,!1,void 0,this),a("span",{style:{color:"#B392F0"},children:"os.path.join(ori_model_dir,"},void 0,!1,void 0,this),a("span",{style:{color:"#9ECBFF"},children:" 'pytorch_model.bin'"},void 0,!1,void 0,this),a("span",{style:{color:"#E1E4E8"},children:"))"},void 0,!1,void 0,this)]},void 0,!0,void 0,this),`
`,a("span",{className:"line",children:[a("span",{style:{color:"#B392F0"},children:"lm_head_w"},void 0,!1,void 0,this),a("span",{style:{color:"#9ECBFF"},children:" ="},void 0,!1,void 0,this),a("span",{style:{color:"#9ECBFF"},children:" model['lm_head.weight']"},void 0,!1,void 0,this)]},void 0,!0,void 0,this),`
`,a("span",{className:"line",children:[a("span",{style:{color:"#B392F0"},children:"lm_head_w"},void 0,!1,void 0,this),a("span",{style:{color:"#9ECBFF"},children:" ="},void 0,!1,void 0,this),a("span",{style:{color:"#9ECBFF"},children:" torch.nn.functional.normalize"},void 0,!1,void 0,this),a("span",{style:{color:"#E1E4E8"},children:"("},void 0,!1,void 0,this),a("span",{style:{color:"#B392F0"},children:"lm_head_w"},void 0,!1,void 0,this),a("span",{style:{color:"#E1E4E8"},children:")"},void 0,!1,void 0,this)]},void 0,!0,void 0,this),`
`,a("span",{className:"line",children:[a("span",{style:{color:"#B392F0"},children:"model["},void 0,!1,void 0,this),a("span",{style:{color:"#B392F0"},children:"'lm_head.weight'"},void 0,!1,void 0,this),a("span",{style:{color:"#B392F0"},children:"]"},void 0,!1,void 0,this),a("span",{style:{color:"#9ECBFF"},children:" ="},void 0,!1,void 0,this),a("span",{style:{color:"#9ECBFF"},children:" lm_head_w"},void 0,!1,void 0,this)]},void 0,!0,void 0,this),`
`,a("span",{className:"line",children:[a("span",{style:{color:"#B392F0"},children:"torch.save(model,"},void 0,!1,void 0,this),a("span",{style:{color:"#9ECBFF"},children:" os.path.join"},void 0,!1,void 0,this),a("span",{style:{color:"#E1E4E8"},children:"("},void 0,!1,void 0,this),a("span",{style:{color:"#B392F0"},children:"new_model_dir,"},void 0,!1,void 0,this),a("span",{style:{color:"#9ECBFF"},children:" 'pytorch_model.bin'"},void 0,!1,void 0,this),a("span",{style:{color:"#E1E4E8"},children:"))"},void 0,!1,void 0,this)]},void 0,!0,void 0,this)]},void 0,!0,void 0,this)},void 0,!1,void 0,this),`
`,a("p",{children:"需要先安装 torch 2.0.1："},void 0,!1,void 0,this),`
`,a("pre",{className:"shiki github-dark",style:{backgroundColor:"#24292e",color:"#e1e4e8"},tabIndex:"0",children:a("code",{className:"language-sh",children:a("span",{className:"line",children:[a("span",{style:{color:"#B392F0"},children:"pip"},void 0,!1,void 0,this),a("span",{style:{color:"#9ECBFF"},children:" install"},void 0,!1,void 0,this),a("span",{style:{color:"#9ECBFF"},children:" torch==="},void 0,!1,void 0,this),a("span",{style:{color:"#79B8FF"},children:"2.0.1"},void 0,!1,void 0,this)]},void 0,!0,void 0,this)},void 0,!1,void 0,this)},void 0,!1,void 0,this),`
`,a("h2",{children:"转换为 gguf 格式"},void 0,!1,void 0,this),`
`,a("p",{children:"llama.cpp 官方已经集成转换脚本，可以在 llama.cpp 中找到："},void 0,!1,void 0,this),`
`,a("pre",{className:"shiki github-dark",style:{backgroundColor:"#24292e",color:"#e1e4e8"},tabIndex:"0",children:a("code",{className:"language-sh",children:a("span",{className:"line",children:[a("span",{style:{color:"#B392F0"},children:"python"},void 0,!1,void 0,this),a("span",{style:{color:"#9ECBFF"},children:" convert-baichuan-hf-to-gguf.py"},void 0,!1,void 0,this),a("span",{style:{color:"#9ECBFF"},children:" ../Baichuan2-7B-Chat-bc1/"},void 0,!1,void 0,this)]},void 0,!0,void 0,this)},void 0,!1,void 0,this)},void 0,!1,void 0,this),`
`,a("h2",{children:"量化"},void 0,!1,void 0,this),`
`,a("pre",{className:"shiki github-dark",style:{backgroundColor:"#24292e",color:"#e1e4e8"},tabIndex:"0",children:a("code",{className:"language-sh",children:a("span",{className:"line",children:[a("span",{style:{color:"#B392F0"},children:"./quantize"},void 0,!1,void 0,this),a("span",{style:{color:"#9ECBFF"},children:" ../Baichuan2-7B-Chat-bc1/ggml-model-f16.gguf"},void 0,!1,void 0,this),a("span",{style:{color:"#9ECBFF"},children:" ../Baichuan2-7B-Chat-bc1/ggml-model-q6_k.gguf"},void 0,!1,void 0,this),a("span",{style:{color:"#9ECBFF"},children:" Q6_K"},void 0,!1,void 0,this)]},void 0,!0,void 0,this)},void 0,!1,void 0,this)},void 0,!1,void 0,this),`
`,a("p",{children:["参考 ",a("a",{href:"https://github.com/ymcui/Chinese-LLaMA-Alpaca-2/wiki/llamacpp_zh",children:"Chinese Llama 2"},void 0,!1,void 0,this)," 的建议，可以使用 Q4_K 或 Q6_K 的量化模式。"]},void 0,!0,void 0,this),`
`,a("p",{children:"baichuan2 7b chat 模型在 Q6_K 的模式下进行量化后，生成的文件大约 5GB。在我的机器上，纯 CPU 推理下可以实现大约 5 token 每秒。硬件配置：i7 12650h（AVX2 指令） + 16g RAM + 1T SSD，Windows 11 系统。5 tps 对于人类阅读刚好够用，对于其他场景属于比较慢。"},void 0,!1,void 0,this),`
`,a("h2",{children:"启动"},void 0,!1,void 0,this),`
`,a("pre",{className:"shiki github-dark",style:{backgroundColor:"#24292e",color:"#e1e4e8"},tabIndex:"0",children:a("code",{className:"language-sh",children:[a("span",{className:"line",children:[a("span",{style:{color:"#B392F0"},children:"./main.exe"},void 0,!1,void 0,this),a("span",{style:{color:"#79B8FF"},children:" \\"},void 0,!1,void 0,this)]},void 0,!0,void 0,this),`
`,a("span",{className:"line",children:[a("span",{style:{color:"#79B8FF"},children:"    -c"},void 0,!1,void 0,this),a("span",{style:{color:"#79B8FF"},children:" 4096"},void 0,!1,void 0,this),a("span",{style:{color:"#79B8FF"},children:" \\"},void 0,!1,void 0,this)]},void 0,!0,void 0,this),`
`,a("span",{className:"line",children:[a("span",{style:{color:"#79B8FF"},children:"    -m"},void 0,!1,void 0,this),a("span",{style:{color:"#9ECBFF"},children:" models/baichuan2-7b-chat-ggml-model-q4_0.gguf"},void 0,!1,void 0,this),a("span",{style:{color:"#79B8FF"},children:" \\"},void 0,!1,void 0,this)]},void 0,!0,void 0,this),`
`,a("span",{className:"line",children:[a("span",{style:{color:"#79B8FF"},children:"    --file"},void 0,!1,void 0,this),a("span",{style:{color:"#9ECBFF"},children:" system_prompt_baichuan2.txt"},void 0,!1,void 0,this),a("span",{style:{color:"#79B8FF"},children:" \\"},void 0,!1,void 0,this)]},void 0,!0,void 0,this),`
`,a("span",{className:"line",children:[a("span",{style:{color:"#79B8FF"},children:"    --logdir"},void 0,!1,void 0,this),a("span",{style:{color:"#9ECBFF"},children:" logs"},void 0,!1,void 0,this),a("span",{style:{color:"#79B8FF"},children:" \\"},void 0,!1,void 0,this)]},void 0,!0,void 0,this),`
`,a("span",{className:"line",children:[a("span",{style:{color:"#79B8FF"},children:"    -i"},void 0,!1,void 0,this),a("span",{style:{color:"#79B8FF"},children:" \\"},void 0,!1,void 0,this)]},void 0,!0,void 0,this),`
`,a("span",{className:"line",children:[a("span",{style:{color:"#79B8FF"},children:"    --color"},void 0,!1,void 0,this),a("span",{style:{color:"#79B8FF"},children:" \\"},void 0,!1,void 0,this)]},void 0,!0,void 0,this),`
`,a("span",{className:"line",children:[a("span",{style:{color:"#79B8FF"},children:"    --reverse-prompt"},void 0,!1,void 0,this),a("span",{style:{color:"#9ECBFF"},children:' "用户："'},void 0,!1,void 0,this)]},void 0,!0,void 0,this)]},void 0,!0,void 0,this)},void 0,!1,void 0,this),`
`,a("p",{children:"system_prompt_baichuan2.txt 来自 llama.cpp 的 prompts/chat-with-baichuan.txt："},void 0,!1,void 0,this),`
`,a("pre",{className:"shiki github-dark",style:{backgroundColor:"#24292e",color:"#e1e4e8"},tabIndex:"0",children:a("code",{className:"language-log",children:[a("span",{className:"line",children:a("span",{style:{color:"#E1E4E8"},children:"以下内容为人类用户与与一位智能助手的对话。"},void 0,!1,void 0,this)},void 0,!1,void 0,this),`
`,a("span",{className:"line"},void 0,!1,void 0,this),`
`,a("span",{className:"line",children:a("span",{style:{color:"#E1E4E8"},children:"用户：你好！"},void 0,!1,void 0,this)},void 0,!1,void 0,this),`
`,a("span",{className:"line",children:a("span",{style:{color:"#E1E4E8"},children:"助手："},void 0,!1,void 0,this)},void 0,!1,void 0,this)]},void 0,!0,void 0,this)},void 0,!1,void 0,this),`
`,a("h2",{children:"启动 API 服务"},void 0,!1,void 0,this),`
`,a("p",{children:"llama.cpp 提供了一个简洁的 server 实现，可以提供 web 体验："},void 0,!1,void 0,this),`
`,a("pre",{className:"shiki github-dark",style:{backgroundColor:"#24292e",color:"#e1e4e8"},tabIndex:"0",children:a("code",{className:"language-sh",children:a("span",{className:"line",children:[a("span",{style:{color:"#B392F0"},children:"./server.exe"},void 0,!1,void 0,this),a("span",{style:{color:"#79B8FF"},children:" -m"},void 0,!1,void 0,this),a("span",{style:{color:"#9ECBFF"},children:" models/baichuan2-7b-chat-ggml-model-q4_0.gguf"},void 0,!1,void 0,this),a("span",{style:{color:"#79B8FF"},children:" -c"},void 0,!1,void 0,this),a("span",{style:{color:"#79B8FF"},children:" 4096"},void 0,!1,void 0,this)]},void 0,!0,void 0,this)},void 0,!1,void 0,this)},void 0,!1,void 0,this),`
`,a("p",{children:["访问 ",a("a",{href:"https://127.0.0.1:8080",children:"https://127.0.0.1:8080"},void 0,!1,void 0,this),"，可以看到一个简单的 web 界面。"]},void 0,!0,void 0,this),`
`,a("hr",{},void 0,!1,void 0,this),`
`,a("ul",{children:[`
`,a("li",{children:"2023-10-24"},void 0,!1,void 0,this),`
`,a("li",{children:"2023-10-24，补充量化、server 的说明"},void 0,!1,void 0,this),`
`]},void 0,!0,void 0,this)]},void 0,!0,void 0,this)},void 0,!1,void 0,this)}export{l as default};
